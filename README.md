¬øQu√© es la Evaluaci√≥n de modelos y m√©tricas de rendimiento.? 

```html
<https://colab.research.google.com/drive/1LE_iqYKa2di0MASjTbpC-1NwOXdFmOqZ#scrollTo=3WHQfwy8l2G5>
```

La **evaluaci√≥n de modelos** es el proceso de cuantificar qu√© tan bien predice un algoritmo de Inteligencia Artificial sobre datos que no ha visto antes. Su objetivo es asegurar que el modelo sea capaz de generalizar y no simplemente de memorizar los datos de entrenamiento (un problema conocido como _overfitting_).

Las **m√©tricas de rendimiento** son las reglas de medida üìè espec√≠ficas que usamos para ponerle una "nota" num√©rica a esa evaluaci√≥n. Dependiendo del tipo de problema, usaremos diferentes m√©tricas para entender los errores del modelo.

Puede enfocarse desde un punto de vista de:

1.¬†¬†¬†¬† **M√©tricas para Clasificaci√≥n**: El modelo predice categor√≠as (ej. ¬øes fraude o no?). Se explora la matriz de confusi√≥n, precisi√≥n y exhaustividad.
Se centra en problemas donde el modelo asigna categor√≠as. Aqu√≠ exploraremos por qu√© la exactitud (**Accuracy**) puede ser enga√±osa si los datos no est√°n equilibrados. Aparecen herramientas como **Matriz de Confusi√≥n**, el **F1-Score** o la **Curva ROC**.

2.¬†¬†¬†¬† **M√©tricas para Regresi√≥n**: El modelo predice valores num√©ricos continuos (ej. el precio de una casa).
Aparecen conceptos como el **Error Absoluto Medio (MAE)**, el **Error Cuadr√°tico Medio (MSE)** y el **Coeficiente de Determinaci√≥n ($R^2$)** para entender cu√°nto se alejan nuestras predicciones de la realidad.

3.¬†¬†¬†¬†**Estrategias de Validaci√≥n**: C√≥mo dividir los datos correctamente usando Python 3 para que la evaluaci√≥n sea justa (Train/Test Split y Validaci√≥n Cruzada).
Antes de medir, hay que saber c√≥mo organizar los datos. Veremos la diferencia entre un simple **Train/Test Split** y t√©cnicas m√°s robustas como la **Validaci√≥n Cruzada (K-Fold Cross Validation)**.

La **evaluaci√≥n en clasificaci√≥n** no se trata solo de ver si el modelo "acierta", sino de entender _c√≥mo_ y _d√≥nde_ se equivoca. üéØ

# M√©tricas para Clasificaci√≥n
### 1. Conceptos Base

- **Predicci√≥n de categor√≠as:** Es el proceso donde el modelo asigna una etiqueta discreta a una entrada (ej. "Spam" o "No Spam"). A diferencia de la regresi√≥n, aqu√≠ no buscamos un n√∫mero, sino una clase. üè∑Ô∏è

- **Exactitud (Accuracy):** Es el porcentaje total de predicciones correctas. Se calcula como:    $$\text{Accuracy} = \frac{\text{Aciertos Totales}}{\text{Total de Casos}}$$
- **Datos equilibrados (balanceados):** Ocurre cuando las clases que queremos predecir tienen un n√∫mero similar de ejemplos. Si tienes 500 fotos de gatos üêà y 500 de perros üêï, tus datos est√°n balanceados. Si tienes 990 de gatos y 10 de perros, est√°n **desbalanceados**, y el Accuracy dejar√° de ser una m√©trica fiable.

### 2. Herramientas de Medici√≥n

- **Matriz de Confusi√≥n:** Es una tabla que muestra los aciertos y errores desglosados en cuatro categor√≠as: **Verdaderos Positivos (TP)**, **Verdaderos Negativos (TN)**, **Falsos Positivos (FP)** y **Falsos Negativos (FN)**. Es el "mapa" de los errores del modelo. 

Para entender estos conceptos, imaginemos un **test m√©dico** üè• para detectar una enfermedad. En este escenario, ser "Positivo" significa tener la enfermedad y ser "Negativo" significa estar sano.

| **Concepto**                    | **Lo que dice el modelo** | **Realidad** | **Resultado**                                                                              |
| ------------------------------- | ------------------------- | ------------ | ------------------------------------------------------------------------------------------ |
| **Verdadero Positivo (TP)** ‚úÖ   | Positivo                  | Positivo     | El test detecta correctamente la enfermedad.                                               |
| **Verdadero Negativo (TN)** üõ°Ô∏è | Negativo                  | Negativo     | El test confirma correctamente que la persona est√° sana.                                   |
| **Falso Positivo (FP)** üîî      | Positivo                  | Negativo     | **Falsa alarma:** El test dice que hay enfermedad, pero la persona est√° sana.              |
| **Falso Negativo (FN)** ‚ö†Ô∏è      | Negativo                  | Positivo     | **Error peligroso:** El test dice que la persona est√° sana, pero en realidad est√° enferma. |

Estos cuatro valores son los "ladrillos" con los que construimos todas las m√©tricas de clasificaci√≥n. Dependiendo del problema, nos preocupar√° m√°s un tipo de error que otro.

- **Precisi√≥n (Precision):** ¬øQu√© tan fiable es el modelo cuando dice que algo es positivo? Responde a: _"De todos los que predije como positivos, ¬øcu√°ntos lo eran realmente?"_ üíé
 
- **Exhaustividad (Recall/Sensitivity):** ¬øQu√© capacidad tiene el modelo para encontrar todos los casos positivos? Responde a: _"De todos los casos que eran realmente positivos, ¬øcu√°ntos logr√© detectar?"_ üîç

- **F1-Score:** Es la media arm√≥nica entre la Precisi√≥n y la Exhaustividad. Es muy √∫til cuando quieres un equilibrio entre ambas y tienes clases desbalanceadas. ‚öñÔ∏è

La **media arm√≥nica** es una m√©trica. Est√° dise√±ada para proporcionar un √∫nico valor que equilibre  un conjunto de datos desbalanceado.
### F√≥rmula Matem√°tica üßÆ

A diferencia del promedio normal (media aritm√©tica), la media arm√≥nica se calcula de la siguiente manera:

$$F1 = 2 \cdot \frac{\text{Precisi√≥n} \cdot \text{Exhaustividad}}{\text{Precisi√≥n} + \text{Exhaustividad}}$$

### ¬øPor qu√© usamos la media arm√≥nica y no la normal? ü§î

La media arm√≥nica **penaliza los valores extremos**.

- Si la precisi√≥n es $1.0$ (perfecta) pero la exhaustividad es $0.0$ (p√©sima), la **media aritm√©tica** te dar√≠a un $0.5$, lo cual parece aceptable.

- Sin embargo, la **media arm√≥nica (F1-Score)** te dar√≠a un $0$, reflejando que el modelo realmente no es √∫til porque falla completamente en una de las dos √°reas.

Para calcular estas m√©tricas, utilizamos estas f√≥rmulas:

#### Precisi√≥n (Precision) üéØ

La precisi√≥n mide qu√© tan "limpias" son nuestras predicciones positivas. Es la proporci√≥n de aciertos positivos sobre **todo lo que el modelo marc√≥ como positivo** (aciertos y errores).

$$Precisi√≥n = \frac{TP}{TP + FP}$$

#### Exhaustividad (Recall) üîç

La exhaustividad (tambi√©n llamada sensibilidad) mide la capacidad del modelo para encontrar **todos** los casos positivos reales. Es la proporci√≥n de aciertos positivos sobre **el total de casos que realmente eran positivos**.

$$Exhaustividad = \frac{TP}{TP + FN}$$

---

### C√≥digo de ejemplo en Python

Ejemplo de **clasificaci√≥n binaria** (por ejemplo, detectar si un mensaje es "Spam" o "No Spam").

```Python
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# 1. Definimos datos de ejemplo
# y_true: Las etiquetas reales (la verdad)
# y_pred: Lo que nuestro modelo de IA ha predicho
# (0 = No Spam, 1 = Spam)
y_true = [0, 1, 0, 0, 1, 1, 0, 1, 1, 1]
y_pred = [0, 1, 0, 0, 0, 1, 1, 1, 1, 1]

# 2. Calculamos la Matriz de Confusi√≥n
# El orden por defecto es: [TN, FP], [FN, TP]
tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

print(f"--- Matriz de Confusi√≥n ---")
print(f"Verdaderos Negativos (TN): {tn}")
print(f"Falsos Positivos (FP): {fp}")
print(f"Falsos Negativos (FN): {fn}")
print(f"Verdaderos Positivos (TP): {tp}\n")

# 3. Calculamos las m√©tricas fundamentales
print(f"--- M√©tricas de Rendimiento ---")
print(f"Exactitud (Accuracy): {accuracy_score(y_true, y_pred):.2f}")
print(f"Precisi√≥n (Precision): {precision_score(y_true, y_pred):.2f}")
print(f"Exhaustividad (Recall): {recall_score(y_true, y_pred):.2f}")
print(f"F1-Score: {f1_score(y_true, y_pred):.2f}")
```

### ¬øQu√© est√° pasando en el c√≥digo? üßê

1. **`y_true` vs `y_pred`**: Comparamos la realidad con la predicci√≥n. F√≠jate que en la lista hay 10 elementos.
2. **`ravel()`**: Es un peque√±o truco de Python para extraer los cuatro valores de la matriz (`tn, fp, fn, tp`) directamente de la tabla que genera `scikit-learn`.
3. **M√©tricas**: Usamos las funciones integradas que aplican las f√≥rmulas matem√°ticas que mencionaste antes ($F1$, $Precisi√≥n$, etc.).

---
##### Matriz de Confusi√≥n üß©

Comparando elemento a elemento `y_true` y `y_pred`:

|**√çndice**|**Real (y_true)**|**Predicci√≥n (y_pred)**|**Resultado**|
|---|---|---|---|
|0, 2, 3|0|0|**3 TN** (Acierto en Negativo)|
|1, 5, 7, 8, 9|1|1|**5 TP** (Acierto en Positivo)|
|6|0|1|**1 FP** (Falsa Alarma)|
|4|1|0|**1 FN** (Se le escap√≥ un positivo)|

- **TP (Verdaderos Positivos):** 5 ‚úÖ
- **TN (Verdaderos Negativos):** 3 üõ°Ô∏è
- **FP (Falsos Positivos):** 1 üîî
- **FN (Falsos Negativos):** 1 ‚ö†Ô∏è

---

### 2. Modelo "Perezoso" y Confianza üí§

Un **modelo perezoso** (o _baseline_) es aquel que no aprende patrones, sino que simplemente predice siempre la clase mayoritaria.

Si en tus datos el 60% es "Spam" y el modelo dice siempre "Spam":

- Su **Accuracy** ser√° del 60%.
- Su **Precisi√≥n** (confianza) cae dr√°sticamente. ¬øPor qu√©? Porque la precisi√≥n mide qu√© tan seguro puedes estar cuando el modelo dice "1". Si el modelo dice "1" para todo, pierde valor. En este caso, de cada 10 veces que dice "Spam", fallar√° 4 veces (los ceros reales).

---

### 3. Informe Autom√°tico en Python üêç

En `scikit-learn`, se pueden obtener todas las m√©tricas (Precisi√≥n, Recall, F1-Score y Accuracy) desglosadas por clase con una sola funci√≥n:

```Python
from sklearn.metrics import classification_report

print(classification_report(y_true, y_pred))
```

##### **Curva ROC** (Receiver Operating Characteristic)
Herramienta visual para evaluar qu√© tan bien un modelo de clasificaci√≥n puede distinguir entre dos clases (como "Sano" vs. "Enfermo") a medida que cambiamos el **umbral de decisi√≥n**. üìà

En lugar de mirar un solo n√∫mero, la curva ROC nos muestra el panorama completo al graficar dos m√©tricas enfrentadas estableciendo relaciones (**Tasas**):

1. **Tasa de Verdaderos Positivos (TPR / Sensibilidad):** De todos los positivos reales, ¬øcu√°ntos detectamos correctamente?

$$TPR = \frac{TP}{TP + FN}$$

2. **Tasa de Falsos Positivos (FPR):** De todos los negativos reales, ¬øcu√°ntos marcamos incorrectamente como positivos?
$$FPR = \frac{FP}{FP + TN}$$

---

### Ejemplo pr√°ctico: Radar de Aviones ‚úàÔ∏è

Dise√±as un radar para detectar aviones enemigos. El radar recibe se√±ales y debes decidir qu√© tan fuerte debe ser la se√±al para activar la alarma (**umbral**):

- **Umbral muy bajo:** El radar es s√∫per sensible. Detectar√°s todos los aviones (TPR = 1.0), pero tambi√©n te dar√°n "falsas alarmas" por bandadas de p√°jaros o nubes (FPR muy alto). Estar√≠as en la esquina superior derecha de la curva. üê¶

- **Umbral muy alto:** El radar es muy estricto. Solo suena si la se√±al es gigante. No tendr√°s falsas alarmas (FPR = 0.0), pero se te escapar√°n aviones reales (TPR bajo). Estar√≠as en la esquina inferior izquierda. üõ°Ô∏è

- **El Modelo Ideal:** Ser√≠a aquel que detecta todos los aviones (TPR = 1.0) sin dar ninguna falsa alarma (FPR = 0.0). En la gr√°fica, esto es la esquina superior izquierda.

El **AUC (√Årea Bajo la Curva)** es el n√∫mero que resume esta gr√°fica. Un AUC de **1.0** es un modelo perfecto, mientras que un **0.5** es como lanzar una moneda al aire (puro azar). üé≤
### 1. ¬øPor qu√© el AUC es mejor que el Accuracy? üìâ

El **Accuracy** es como un profesor que solo cuenta cu√°ntas respuestas est√°n bien, sin mirar si las preguntas eran f√°ciles o dif√≠ciles. Si en un examen de 100 preguntas, 99 son de "sumar 1+1" y solo 1 es de "c√°lculo avanzado", alguien que no sepa nada de c√°lculo pero s√≠ de sumas sacar√° un 99% de nota. ¬øEs un experto? No, solo aprovech√≥ el **desbalance de los datos**.

El **AUC (Area Under the Curve)**, en cambio, mide la capacidad del modelo para **ordenar** las probabilidades. Eval√∫a si el modelo es capaz de poner a los "positivos" por encima de los "negativos" en una lista de probabilidades, sin importar d√≥nde pongamos el punto de corte (umbral).

- **Accuracy:** Se ve afectado por la proporci√≥n de clases.
- **AUC:** Es robusto frente al desbalance porque mira el rendimiento en todos los umbrales posibles.
### 2. Umbral de Decisi√≥n y la Curva ROC üéöÔ∏è

Por defecto, un modelo suele decir que algo es "Positivo" si su probabilidad es mayor a **0.5**. Pero ese n√∫mero no es sagrado.

Imagina un detector de incendios üöí:

- Si bajas el **umbral** a 0.1, el sensor saltar√° con el m√≠nimo humo (mucha **Exhaustividad/Recall**), pero tendr√°s muchas falsas alarmas (Falsos Positivos).
- Si subes el **umbral** a 0.9, solo sonar√° si hay una hoguera en la cocina (mucha **Precisi√≥n**), pero quiz√°s se te queme la casa antes de que suene (Falsos Negativos).
### 3. C√≥digo en Python 3 üêç

```Python
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score

# 1. Generamos datos desbalanceados (90% clase 0, 10% clase 1)
X, y = make_classification(n_samples=1000, n_classes=2, weights=[0.9, 0.1], random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 2. Entrenamos un modelo simple
model = LogisticRegression()
model.fit(X_train, y_train)

# 3. Obtenemos las probabilidades (necesarias para la curva ROC)
# Tomamos la columna [:, 1] que es la probabilidad de ser clase "1"
probs = model.predict_proba(X_test)[:, 1]

# 4. Calculamos el AUC y la curva
auc = roc_auc_score(y_test, probs)
fpr, tpr, thresholds = roc_curve(y_test, probs)

# 5. Visualizaci√≥n
plt.figure(figsize=(7, 5))
plt.plot(fpr, tpr, label=f'Modelo (AUC = {auc:.2f})')
plt.plot([0, 1], [0, 1], 'k--', label='Azar (AUC = 0.50)') # L√≠nea de referencia
plt.xlabel('Tasa de Falsos Positivos (FPR)')
plt.ylabel('Tasa de Verdaderos Positivos (TPR)')
plt.title('Curva ROC')
plt.legend()
plt.grid(True)
plt.show()
```

#### Planteamiento:
Para el dise√±o de un sistema para detectar **fraude bancario** üí≥ (donde el 99.9% de las transacciones son legales y solo el 0.1% son fraude), ¬øqu√© pasar√≠a si se usara un umbral muy alto (0.95)? ¬øEstar√≠a el banco m√°s preocupado por molestar a clientes inocentes o por dejar pasar a un estafador?
¬øQu√© m√©trica ser√≠a prioritaria en ese caso?

# M√©tricas para Regresi√≥n

Pasamos ahora al terreno de la **Regresi√≥n** üìà. A diferencia de la clasificaci√≥n, donde se buscan etiquetas, aqu√≠ el objetivo es predecir un valor num√©rico continuo (como el precio de una vivienda o la temperatura).

En este contexto, la evaluaci√≥n se basa en medir la **"distancia"** o el error entre el valor real ($y$) y la predicci√≥n del modelo ($\hat{y}$). Vamos a explorar estas m√©tricas clave:

### 1. Error Absoluto Medio (MAE) üìè

Es la m√©trica m√°s intuitiva. Simplemente calcula el promedio de las diferencias absolutas entre la realidad y la predicci√≥n.

- **F√≥rmula:** $\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$

- **En lenguaje sencillo:** "En promedio, ¬øcu√°ntas unidades se equivoca mi modelo?". Si predices precios de casas y el MAE es 5.000‚Ç¨, significa que tus predicciones suelen errar por esa cantidad.

### 2. Error Cuadr√°tico Medio (MSE) üü•

Similar al MAE, pero eleva los errores al cuadrado antes de promediarlos.

- **F√≥rmula:** $\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$

- **Importancia:** Al elevar al cuadrado, el MSE **penaliza mucho m√°s los errores grandes**. Es muy √∫til si en tu problema un error peque√±o es aceptable, pero uno grande es un desastre.


### 3. Coeficiente de Determinaci√≥n ($R^2$) üìä

A diferencia de las anteriores, esta m√©trica no est√° en las unidades de los datos, sino que suele ir de 0 a 1 (o incluso valores negativos).

- **Interpretaci√≥n:** Indica qu√© porcentaje de la variaci√≥n de los datos es "explicado" por el modelo.

   - **$R^2 = 1$:** Predicci√≥n perfecta.
   - **$R^2 = 0$:** Tu modelo es tan √∫til como una l√≠nea horizontal que siempre predice el promedio.   
### Relaci√≥n con la "Realidad" üåç

Podemos resumir la relaci√≥n de estas m√©tricas con la precisi√≥n de la siguiente manera:

| **M√©trica** | **Si las predicciones se ALEJAN üöÄ<br>de la realidad...**             | **Si las predicciones se ACERCAN üéØ<br>a la realidad...** |
| ----------- | --------------------------------------------------------------------- | --------------------------------------------------------- |
| **MAE**     | El valor **sube** (mayor error promedio).                             | El valor **baja** (hacia 0).                              |
| **MSE**     | El valor **sube dr√°sticamente** (especialmente con valores at√≠picos). | El valor **baja** r√°pidamente.                            |
| **$R^2$**   | El valor **baja** (se acerca a 0 o se vuelve negativo).               | El valor **sube** (se acerca a 1).                        |

Nota: No s√© como formatear la tabla para:
asignar un 20% del ancho de p√°gina a la primera columna.
asignar un 40% del ancho de p√°gina a la segunda, y , a la tercera y √∫ltima columna.
agregar saltos de l√≠neas dentro de las celdas de forma que los par√©ntesis queden en la misma celda pero en diferente l√≠nea.
Hacer que se lea **$R^2$** con ese formato de base y super√≠ndice.
etc., etc.
He intentado hacerlo con Obsidian, Gemini, Hojas de C√°lculo de Google y Word pero es un l√≠o.
Adem√°s si le introduzco a Gemini una entrada en la que le intento especificar lo que quiero, le echa "imaginaci√≥n"...
Me da que hay demasiada tela....

Cu√°ndo usar **MAE** o **MSE**, frente a un error grande o **outlier** üöÄ. Imagina que estamos entrenando un modelo para predecir el precio de 3 casas (en miles de euros).

### 1. F√≥rmulas y Escenario

Usaremos estos datos donde la tercera predicci√≥n es un "valor extra√±o" (se equivoca por mucho):

- **Valores reales ($y$):** $[200, 300, 400]$
- **Predicciones ($\hat{y}$):** $[210, 290, 460]$ (Errores: $+10, -10, +60$)
### 2. C√°lculo de MAE (Error Absoluto Medio) üìè

El MAE trata a todos los errores por igual, de forma lineal.

$$MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|$$
**Sustituci√≥n:**
$$MAE = \frac{|200-210| + |300-290| + |400-460|}{3} = \frac{10 + 10 + 60}{3} = \mathbf{26.6}$$
### 3. C√°lculo de MSE (Error Cuadr√°tico Medio) üü•

El MSE eleva los errores al cuadrado, lo que amplifica los fallos grandes.

$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$
**Sustituci√≥n:**
$$MSE = \frac{(200-210)^2 + (300-290)^2 + (400-460)^2}{3} = \frac{100 + 100 + 3600}{3} = \mathbf{1266.6}$$
### Razonamiento e Interpretaci√≥n üß†

- **Efecto del Outlier:** En el MAE, el error de "60" pesa lo que vale. Pero en el MSE, ese mismo error se convierte en **3600**. El MSE "castiga" con mucha m√°s dureza las predicciones que se alejan mucho de la realidad.
- **¬øCu√°ndo usar cu√°l?:**

- Usa **MAE** si quieres una medida robusta que no se vea alterada dr√°sticamente por valores at√≠picos (es m√°s "tolerante").  
- Usa **MSE** si en tu proyecto un error grande es inaceptable y quieres que el modelo aprenda a evitarlos a toda costa.

Caso pr√°ctico: **Sistema de frenado de un coche aut√≥nomo üöó**. 
Si el sistema calcula mal la distancia por un margen peque√±o, no pasa mucho, pero si se equivoca por mucho, el riesgo es total. 

**Coeficiente de Determinaci√≥n o $R^2$**  A diferencia del MAE o el MSE que nos dan errores en unidades espec√≠ficas, el $R^2$ nos da una medida relativa de la calidad del modelo. üìä
### 1. ¬øQu√© es el $R^2$ y cu√°l es su f√≥rmula? üßê

El $R^2$ mide qu√© proporci√≥n de la **varianza** total de los datos es explicada por nuestro modelo. Es como comparar nuestro modelo contra un "modelo base" muy simple: uno que siempre predice la **media** ($\bar{y}$) de los datos.

La f√≥rmula es:

$$R^2 = 1 - \frac{SS_{res}}{SS_{tot}}$$

Donde:

- **$SS_{res}$ (Suma de Cuadrados de los Residuos):** Es la suma de los errores de nuestro modelo al cuadrado. $\sum (y_i - \hat{y}_i)^2$. F√≠jate que esto es b√°sicamente el **MSE** multiplicado por el n√∫mero de datos.
- **$SS_{tot}$ (Suma de Cuadrados Totales):** Es la varianza total de los datos respecto a su media. $\sum (y_i - \bar{y})^2$. Representa el error que cometer√≠amos si no tuvi√©ramos modelo y solo us√°ramos el promedio.
### 2. Relaci√≥n con la "Inteligencia" del Modelo üß†

El valor de $R^2$ nos dice cu√°nta informaci√≥n √∫til ha capturado el modelo frente a simplemente "adivinar" el promedio.

|**Valor de R2**|**Interpretaci√≥n de la "Inteligencia"**|**Significado t√©cnico**|
|---|---|---|
|**Cercano a 1**|**Modelo Brillante** ‚ú®|El modelo explica casi toda la variabilidad. Las predicciones est√°n muy cerca de los valores reales.|
|**Cercano a 0**|**Modelo "Perezoso"** üí§|El modelo no es mejor que predecir siempre la media. No ha aprendido ninguna relaci√≥n √∫til entre las variables.|
|**Negativo**|**Modelo "Da√±ino"** ‚ö†Ô∏è|¬°Incre√≠ble pero posible! Significa que el modelo es **peor** que predecir la media. Sus predicciones est√°n tan alejadas que confunden m√°s de lo que ayudan.|
### 3. Ejemplo Num√©rico y el efecto de los Outliers üöÄ

Tenemos 3 datos de ventas: **10, 20, 30**. La media ($\bar{y}$) es **20**.
Calculamos la variabilidad total ($SS_{tot}$):
$$(10-20)^2 + (20-20)^2 + (30-20)^2 = 100 + 0 + 100 = \mathbf{200}$$
#### Caso A: Modelo Bueno
Predicciones: **11, 19, 31**.
$SS_{res} = (10-11)^2 + (20-19)^2 + (30-31)^2 = 1 + 1 + 1 = \mathbf{3}$
$$R^2 = 1 - \frac{3}{200} = \mathbf{0.985}$$
#### Caso B: Modelo con Outlier (Valor extra√±o)
Supongamos que el modelo predice bien los dos primeros, pero en el tercero falla mucho por un outlier: **11, 19, 60**.
$SS_{res} = (10-11)^2 + (20-19)^2 + (30-60)^2 = 1 + 1 + 900 = \mathbf{902}$

$$R^2 = 1 - \frac{902}{200} = \mathbf{-3.51}$$
**Razonamiento:** El outlier dispar√≥ el error al cuadrado ($SS_{res}$), haciendo que el numerador sea mucho mayor que el denominador. Como resultado, el $R^2$ se vuelve negativo, indicando que el modelo ha "perdido el norte".
### 1. Varianza Total ($SS_{tot}$) üìâ

La **Suma de Cuadrados Totales** ($SS_{tot}$) representa la variabilidad natural de tus datos. Es el error que cometer√≠as si no usaras ning√∫n modelo y simplemente predijeras siempre la **media** ($\bar{y}$).
**F√≥rmula:**
$$SS_{tot} = \sum_{i=1}^{n} (y_i - \bar{y})^2$$
**Ejemplo Num√©rico:**
Imagina que tenemos estos precios de casas (en miles): $y = [100, 200, 300]$.
1. Calculamos la media: $\bar{y} = \frac{100+200+300}{3} = 200$.
2. Sustituimos en la f√≥rmula:
$$SS_{tot} = (100 - 200)^2 + (200 - 200)^2 + (300 - 200)^2$$    $$SS_{tot} = (-100)^2 + 0^2 + 100^2 = 10,000 + 0 + 10,000 = \mathbf{20,000}$$
### 2. Interpretaci√≥n Combinada: $R^2$ + MAE üîç

| **Escenario**             | **R2** | **MAE** | **Conclusi√≥n**                                                                            |
| ------------------------- | ------ | ------- | ----------------------------------------------------------------------------------------- |
| **Error Generalizado**    | Bajo   | Alto    | El modelo no entiende la tendencia general de los datos. ‚ùå                                |
| **Presencia de Outliers** | Alto   | Bajo    | El modelo predice bien la mayor√≠a de casos, pero falla estrepitosamente en unos pocos. üöÄ |

**¬øPor qu√© sucede esto?** El MAE no se "asusta" con los errores grandes (outliers), pero el $R^2$ (que usa errores al cuadrado) cae r√°pidamente cuando hay una predicci√≥n muy lejana.
### 3. Limitaciones y $R^2$ Ajustado ‚öñÔ∏è

El gran peligro del **Overfitting** (sobreajuste) con el $R^2$ es que, si a√±ades m√°s variables a tu modelo (aunque sean ruido o datos sin sentido), el $R^2$ **nunca bajar√°**. Esto da una falsa sensaci√≥n de mejora.

Para corregir esto, usamos el **$R^2$ Ajustado**:

- **¬øQu√© hace?** Penaliza al modelo por cada variable adicional que no aporta valor real. üìâ
- **¬øPor qu√© se usa?** Para saber si una variable nueva realmente mejora la predicci√≥n o si solo estamos "memorizando" el ruido.

El **$R^2$ ajustado** es una versi√≥n modificada del coeficiente de determinaci√≥n que introduce una "penalizaci√≥n" por la complejidad del modelo. A diferencia del $R^2$ convencional, que siempre aumenta (o se mantiene igual) al a√±adir nuevas variables ‚Äîaunque estas sean puro ruido‚Äî, el $R^2$ ajustado solo sube si la nueva variable mejora el modelo m√°s de lo que se esperar√≠a por puro azar.
### ¬øPor qu√© se usa con muchas variables? ‚öñÔ∏è

En ciencia de datos, existe el riesgo de caer en el **overfitting** (sobreajuste). Si a√±adimos suficientes variables (por ejemplo, el signo del zod√≠aco del vendedor para predecir el precio de una casa), el $R^2$ normal subir√° ligeramente por pura coincidencia matem√°tica. El $R^2$ ajustado detecta este "enga√±o" y reduce su valor para reflejar que el modelo se est√° volviendo innecesariamente complejo sin ganar precisi√≥n real.
### F√≥rmula Matem√°tica üßÆ

$$\bar{R}^2 = 1 - \left[ \frac{(1 - R^2)(n - 1)}{n - k - 1} \right]$$

Donde:

- **$R^2$**: Coeficiente de determinaci√≥n normal.
- **$n$**: N√∫mero de observaciones (tama√±o de la muestra).
- **$k$**: N√∫mero de variables predictoras (caracter√≠sticas).
### Ejemplo Num√©rico y Sustituci√≥n üî¢

Estamos prediciendo el precio de unas viviendas con un dataset peque√±o:

- **Muestra ($n$):** 20 casas.
- **Modelo A (2 variables):** $k = 2$, $R^2 = 0.80$.
- **Modelo B (A√±adimos 10 variables in√∫tiles):** $k = 12$, $R^2$ sube a $0.82$ por azar.

#### Sustituci√≥n para el Modelo A:

$$\bar{R}^2 = 1 - \left[ \frac{(1 - 0.80)(20 - 1)}{20 - 2 - 1} \right] = 1 - \left[ \frac{0.20 \cdot 19}{17} \right] = 1 - 0.223 \approx \mathbf{0.777}$$
#### Sustituci√≥n para el Modelo B:

$$\bar{R}^2 = 1 - \left[ \frac{(1 - 0.82)(20 - 1)}{20 - 12 - 1} \right] = 1 - \left[ \frac{0.18 \cdot 19}{7} \right] = 1 - 0.488 \approx \mathbf{0.512}$$
### Interpretaci√≥n de los resultados üß†

Aunque el $R^2$ del Modelo B es mayor ($0.82$ frente a $0.80$), su **$R^2$ ajustado se desploma** de $0.777$ a $0.512$.

Esto nos indica que las 10 variables extra no est√°n aportando valor real; al contrario, est√°n "inflando" artificialmente la m√©trica de rendimiento y restando fiabilidad al modelo. El $R^2$ ajustado nos dice que el Modelo A es, en realidad, mucho m√°s "inteligente" y robusto.

| **Concepto Te√≥rico**                  | **Siglas**      | **Funci√≥n en sklearn.metrics**                                         |
| ------------------------------------- | --------------- | ---------------------------------------------------------------------- |
| Error Absoluto Medio                  | **MAE**         | `mean_absolute_error`                                                  |
| Error Cuadr√°tico Medio                | **MSE**         | `mean_squared_error`                                                   |
| Ra√≠z del Error Cuadr√°tico Medio       | **RMSE**        | `root_mean_squared_error` (o `mean_squared_error` con `squared=False`) |
| Coeficiente de Determinaci√≥n          | **$R^2$**       | `r2_score`                                                             |
| Coeficiente de Determinaci√≥n Ajustado | **$\bar{R}^2$** | _(No existe funci√≥n directa, se calcula manualmente)_                  |
### Notas üìù

**RMSE:** Es muy com√∫n incluirlo porque, al ser la ra√≠z cuadrada del MSE, devuelve el error a las **unidades originales** de los datos (como euros üí∂ o metros üìè), pero manteniendo la penalizaci√≥n a los errores grandes.
**$R^2$ Ajustado:** En el ecosistema de Machine Learning con `scikit-learn`, se prioriza el rendimiento predictivo. El $R^2$ ajustado es m√°s frecuente en el an√°lisis estad√≠stico cl√°sico (donde se usa la librer√≠a `statsmodels`), por lo que en Python se suele programar la f√≥rmula usando los resultados de `r2_score`.

Ejemplo de script con combinaci√≥n de c√°lculo y visualizaci√≥n. C√≥digo en **Python 3** que:
1. Genera datos sint√©ticos (simulando precios de casas seg√∫n su tama√±o).
2. Entrena un modelo simple.
3. Calcula todas las m√©tricas solicitadas.
4. Genera un gr√°fico explicativo donde se ven los "residuos" (las l√≠neas rojas que representan el error).
Copiar y pegar este bloque Ejemplo en Notebook (Jupyter/Colab).

### C√≥digo Python: M√©tricas de Regresi√≥n üìâ

```Python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# --- 1. GENERACI√ìN DE DATOS (Dataset Inline) ---
# Simulamos datos: X = Tama√±o (m2), y = Precio (miles de ‚Ç¨)
np.random.seed(42) # Semilla para reproducibilidad
X = 2 * np.random.rand(20, 1) # 20 casas aleatorias
y = 4 + 3 * X + np.random.randn(20, 1) # Precio con un poco de "ruido" (varianza natural)

# --- 2. ENTRENAMIENTO DEL MODELO ---
model = LinearRegression()
model.fit(X, y)
y_pred = model.predict(X) # Predicciones del modelo

# --- 3. C√ÅLCULO DE M√âTRICAS ---

# A) MAE (Error Absoluto Medio)
# Promedio de la diferencia absoluta.
mae = mean_absolute_error(y, y_pred)

# B) MSE (Error Cuadr√°tico Medio)
# Promedio de los errores al cuadrado (penaliza mucho los fallos grandes).
mse = mean_squared_error(y, y_pred)

# C) RMSE (Ra√≠z del Error Cuadr√°tico Medio)
# Devuelve el error a las unidades originales (miles de ‚Ç¨).
# Nota: En sklearn versiones nuevas existe root_mean_squared_error, 
# pero la forma m√°s compatible es la ra√≠z del MSE.
rmse = np.sqrt(mse) 

# D) R2 (Coeficiente de Determinaci√≥n)
# Qu√© porcentaje de la varianza explicamos (0 a 1).
r2 = r2_score(y, y_pred)

# E) R2 Ajustado (C√°lculo manual)
# Penaliza si a√±adimos variables in√∫tiles.
n = len(y)        # N√∫mero de muestras (20)
p = X.shape[1]    # N√∫mero de variables predictoras (1)
r2_ajustado = 1 - (1 - r2) * (n - 1) / (n - p - 1)

# --- 4. IMPRESI√ìN DE RESULTADOS ---
print(f"--- üìä REPORTE DE M√âTRICAS DE REGRESI√ìN ---")
print(f"MAE (Error Absoluto):      {mae:.4f} (El error promedio es de {mae:.2f} miles de ‚Ç¨)")
print(f"MSE (Error Cuadr√°tico):    {mse:.4f} (Dif√≠cil de interpretar directamente)")
print(f"RMSE (Ra√≠z del MSE):       {rmse:.4f} (El error est√°ndar es de {rmse:.2f} miles de ‚Ç¨)")
print(f"R¬≤ (Score):                {r2:.4f} (El modelo explica el {r2*100:.1f}% de la varianza)")
print(f"R¬≤ Ajustado:               {r2_ajustado:.4f} (Ajuste por complejidad del modelo)")

# --- 5. VISUALIZACI√ìN GR√ÅFICA ---
plt.figure(figsize=(10, 6))

# a) Dibujar los datos reales
plt.scatter(X, y, color='blue', label='Datos Reales (y)')

# b) Dibujar la l√≠nea de regresi√≥n (predicci√≥n)
plt.plot(X, y_pred, color='green', linewidth=2, label='Modelo / Predicci√≥n (≈∑)')

# c) Dibujar los ERRORES (Residuos)
# Estas l√≠neas rojas son lo que miden MAE y MSE
for i in range(len(X)):
    plt.plot([X[i], X[i]], [y[i], y_pred[i]], color='red', linestyle='--', alpha=0.5)

# Decoraci√≥n del gr√°fico
plt.title('Regresi√≥n Lineal: Visualizando los Errores (Residuos)', fontsize=14)
plt.xlabel('Tama√±o de la casa (X)')
plt.ylabel('Precio (y)')
plt.legend()
plt.grid(True, alpha=0.3)

# Mostrar solo la primera leyenda de "Error" para no saturar
plt.plot([], [], color='red', linestyle='--', label='Error (Residuo)')
plt.legend()

plt.show()
```

### Interpretaci√≥n de este c√≥digo üßê

1. **Consola:** Ver√°n los n√∫meros exactos. F√≠jate en la diferencia entre el **MSE** (que sale alto, por ejemplo 0.6 o 0.8) y el **RMSE** (que baja a 0.8 o 0.9). Esto ilustra por qu√© el RMSE es m√°s f√°cil de "leer" (est√° en euros, no en "euros cuadrados").
2. **Gr√°fico:**
- Los **puntos azules** son la realidad.
   - La **l√≠nea verde** es el modelo.
   - Las **l√≠neas rojas discontinuas** son clave: representan la distancia $y - \hat{y}$.
   - El **MAE** es el promedio de la longitud de esas l√≠neas rojas.
   - El **MSE** es el promedio de la longitud de esas l√≠neas elevadas al cuadrado (dando m√°s peso a las l√≠neas largas).

Adici√≥n de un **outlier** artificial al c√≥digo (un punto muy lejano) para que vean en directo c√≥mo el $R^2$ se desploma y el MSE se dispara.

Un solo punto puede destruir un modelo lo que refleja la importancia de limpiar los datos.

C√≥digo listo para copiar y pegar en una **nueva celda** de Google Colab o Jupyter Notebook. Se asume que ya se ha ejecutado el c√≥digo anterior (variables `X`, `y` y `model` originales).
### C√≥digo: El Efecto del Outlier (Destruyendo el Modelo) üí•

```Python
# --- 6. EL EXPERIMENTO DEL OUTLIER (A√±adimos un dato "t√≥xico") ---

# 1. Crear el Outlier
# A√±adimos un punto en X=3 (lejos) con valor Y=-5 (muy negativo, rompiendo la tendencia positiva)
X_outlier = np.vstack([X, [[3.0]]])
y_outlier = np.vstack([y, [[-5.0]]])

# 2. Entrenar un NUEVO modelo con el dato contaminado
model_bad = LinearRegression()
model_bad.fit(X_outlier, y_outlier)
y_pred_bad = model_bad.predict(X_outlier)

# 3. Recalcular m√©tricas para ver el desastre
mse_bad = mean_squared_error(y_outlier, y_pred_bad)
r2_bad = r2_score(y_outlier, y_pred_bad)

# --- 4. COMPARATIVA DE IMPACTO ---
print(f"--- üö® IMPACTO DEL OUTLIER üö® ---")
print(f"MSE Original: {mse:.2f}  --->  MSE con Outlier: {mse_bad:.2f} (¬°Se ha disparado!)")
print(f"R¬≤ Original:  {r2:.2f}  --->  R¬≤ con Outlier:  {r2_bad:.2f} (El modelo ha empeorado dr√°sticamente)")

# --- 5. VISUALIZACI√ìN DEL DA√ëO ---
plt.figure(figsize=(10, 6))

# a) Datos originales (Azul) y el Outlier (Rojo Gigante)
plt.scatter(X, y, color='blue', alpha=0.5, label='Datos Normales')
plt.scatter([3.0], [-5.0], color='red', s=200, marker='X', label='OUTLIER (Dato Extra√±o)')

# b) L√≠nea del modelo ORIGINAL (Punteada Verde) - Lo que deber√≠a ser
X_range = np.linspace(0, 3.5, 100).reshape(-1, 1)
plt.plot(X_range, model.predict(X_range), color='green', linestyle='--', linewidth=2, label='Modelo Original (Sin Outlier)')

# c) L√≠nea del modelo AFECTADO (S√≥lida Roja) - C√≥mo el outlier "tira" de la l√≠nea
plt.plot(X_range, model_bad.predict(X_range), color='red', linewidth=3, label='Modelo Afectado (Sesgado)')

# Decoraci√≥n
plt.title('C√≥mo un solo Outlier "rompe" la Regresi√≥n y dispara el MSE', fontsize=14)
plt.xlabel('Tama√±o (X)')
plt.ylabel('Precio (y)')
plt.legend()
plt.grid(True, alpha=0.3)

# Mostrar la distancia del error del outlier (l√≠nea vertical negra)
plt.plot([3.0, 3.0], [-5.0, model_bad.predict([[3.0]])[0][0]], color='black', linestyle=':', label='Error del Outlier')

plt.show()
```
### Salida esperada üëÄ

1. **Consola:** **MSE** se multiplica (quiz√°s pase de 0.8 a 15.0 o m√°s) debido a que ese √∫nico error se eleva al cuadrado. El **$R^2$** probablemente caiga en picado (incluso podr√≠a volverse negativo o muy cercano a 0), indicando que el modelo ya no explica bien la varianza general.
2. **Gr√°fico:**
   - L√≠nea verde (el modelo bueno) siguiendo a la mayor√≠a de los puntos.
   - **L√≠nea roja** (el modelo nuevo) inclinada hacia abajo a la derecha, "secuestrada" por el punto rojo (la X grande).
   - Esto demuestra gr√°ficamente que el MSE (y la regresi√≥n lineal simple) es muy sensible a los valores at√≠picos.

# Estrategias de Validaci√≥n. üõ°Ô∏è

En el contexto de Machine Learning con Python, la "validaci√≥n" no se refiere a comprobar si un dato es un n√∫mero o una letra (eso es limpieza de datos). Aqu√≠, **validar** significa someter al modelo a un examen para asegurarnos de que funcionar√° bien en el mundo real y que no ha memorizado las respuestas (overfitting).
### 1. Train/Test Split (Dividir en Entrenamiento y Prueba) ‚úÇÔ∏è

Es la estrategia m√°s sencilla y r√°pida. Consiste en dividir el conjunto de datos.

- **Training Set (Entrenamiento):** Suele ser el 70-80% de los datos. Es el "libro de texto" con el que el modelo estudia.
- **Test Set (Prueba):** Es el 20-30% restante. Es el "examen final". El modelo **nunca** ve estos datos durante el entrenamiento. Solo los usamos al final para evaluarlo.

**El riesgo:** Depende del azar, si el "Test" contiene solo casos f√°ciles o si contiene solo casos dif√≠ciles, se puede producir una falsa sensaci√≥n de ajuste o desajuste.

### 2. Validaci√≥n Cruzada (K-Fold Cross Validation) üîÑ

Esta es una estrategia m√°s robusta y "democr√°tica". En lugar de hacer un solo examen, hacemos $K$ ex√°menes diferentes.

1. Dividimos los datos en **$K$ partes iguales** (o "folds"). Por ejemplo, $K=5$.
2. Iteraci√≥n 1: Usamos la parte 1 para probar y las otras 4 para entrenar.
3. Iteraci√≥n 2: Usamos la parte 2 para probar y las otras 4 para entrenar.
4. ... Repetimos hasta usar todas las partes como prueba una vez.
5. **Resultado Final:** Promediamos las 5 notas obtenidas.

**La ventaja:** Eliminamos el factor suerte. Todos los datos son usados para entrenar y para probar en alg√∫n momento. El resultado es mucho m√°s fiable.
### Tabla Comparativa: Train/Test vs. K-Fold ü•ä

Tabla resumen:

| **Caracter√≠stica**  | **Train/Test Split**                                        | **K-Fold Cross Validation**                                        |
| ------------------- | ----------------------------------------------------------- | ------------------------------------------------------------------ |
| **Velocidad** ‚ö°     | **Muy r√°pido** (se entrena 1 vez).                          | **Lento** (se entrena $K$ veces).                                  |
| **Fiabilidad** üõ°Ô∏è  | Baja (depende de c√≥mo caiga el corte).                      | **Alta** (el promedio suaviza la suerte).                          |
| **Uso de datos** üíæ | Desperdicia datos (el Test set nunca se usa para aprender). | Eficiente (todos los datos sirven para aprender en alg√∫n momento). |
| **Cu√°ndo usarlo**   | Datasets muy grandes (donde K-Fold ser√≠a eterno).           | Datasets peque√±os o medianos (donde cada dato cuenta).             |
### Ejemplo en Python 3 üêç

Este c√≥digo ilustra ambas t√©cnicas usando `scikit-learn`.

```Python
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression

# 1. Generamos datos de ejemplo (100 filas)
X, y = make_classification(n_samples=100, random_state=42)

# --- ESTRATEGIA A: TRAIN/TEST SPLIT ---
# Dividimos una sola vez: 80% estudiar, 20% examen
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model_a = LogisticRegression()
model_a.fit(X_train, y_train)
score_a = model_a.score(X_test, y_test)

print(f"--- Estrategia Train/Test ---")
print(f"Nota del examen √∫nico: {score_a:.2f} (Puede ser suerte)")

# --- ESTRATEGIA B: CROSS VALIDATION (K-Fold) ---
# K=5: Haremos 5 ex√°menes diferentes
model_b = LogisticRegression()
# cross_val_score hace todo el trabajo sucio por nosotros
scores_b = cross_val_score(model_b, X, y, cv=5)

print(f"\n--- Estrategia K-Fold (K=5) ---")
print(f"Notas de los 5 ex√°menes: {scores_b}")
print(f"Nota PROMEDIO real: {scores_b.mean():.2f} (M√°s fiable)")
```
### Conclusiones üéì

Si existe la posibilidad y el dataset no es gigantesco (millones de datos), **siempre** intentar usar **Cross Validation** para asegurar un modelo estable. 
El _Train/Test Split_ prototipar r√°pido.
_K-Fold_ para validar con mayor fiabilidad.

## ¬øC√≥mo guardar el modelo una vez validado?

Se ha entrenando y validando el modelo (horas o d√≠as). Ha aprendido patrones complejos y tiene un **Accuracy** o un **$R^2$** incre√≠ble. Si se cierra el cuaderno de Python (Jupyter/Colab), **toda esa "inteligencia" se pierde** en la memoria RAM y ser√≠a necesario re-entrenar desde cero la pr√≥xima vez.

Para evitar esto, usamos la **Serializaci√≥n**. Es el proceso de guardar el modelo en un archivo (como si fuera un documento de Word o una partida guardada de un videojuego üéÆ) para poder usarlo despu√©s en producci√≥n o en otra aplicaci√≥n.

En **Python 3** y `scikit-learn`, la herramienta est√°ndar y m√°s eficiente es **`joblib`**.

### C√≥digo: Guardar y Cargar (receta final) üíæ

Se muestra c√≥mo guardar el modelo entrenado de la IA en un archivo `.pkl` y c√≥mo recargar m√°s tarde.

```Python
import joblib
from sklearn.linear_model import LinearRegression

# 1. Supongamos que este es tu modelo YA ENTRENADO y VALIDADO
# (Usamos un ejemplo simple)
modelo = LinearRegression()
X = [[1], [2], [3]]
y = [2, 4, 6]
modelo.fit(X, y)

print("‚úÖ Modelo entrenado. Predicci√≥n para 5: ", modelo.predict([[5]]))

# --- PASO A: GUARDAR EL MODELO (Serializaci√≥n) ---
# Usamos joblib.dump(objeto, 'nombre_archivo.pkl')
filename = 'mi_super_modelo_v1.pkl'
joblib.dump(modelo, filename)

print(f"üíæ El modelo se ha guardado exitosamente en '{filename}'")
print("... Simulamos que cerramos el programa y pasa el tiempo ...\n")

# --- PASO B: CARGAR EL MODELO (Deserializaci√≥n) ---
# Ahora puedes estar en otro script, otro d√≠a, o en un servidor web.
# No necesitas tener los datos de entrenamiento (X, y) originales, solo el archivo.

modelo_cargado = joblib.load(filename)

print("üìÇ Modelo cargado desde el disco.")

# --- PASO C: USARLO EN PRODUCCI√ìN ---
# El modelo cargado recuerda todo lo que aprendi√≥.
nueva_prediccion = modelo_cargado.predict([[5]])
print(f"üîÆ Predicci√≥n del modelo cargado para 5: {nueva_prediccion}")
```
### Resumen Final 

1. **Clasificaci√≥n:**  _Accuracy_ puede ser confuso,  mejor _Matriz de Confusi√≥n_, _F1-Score_ y la _Curva ROC_.
2. **Regresi√≥n:**  _MAE_ (robusto) y _MSE_ (sensible a outliers),  _R^2_ para medir el ajuste frente al promedio.
3. **Validaci√≥n:** _Train/Test Split_ es r√°pido pero arriesgado, y _Cross Validation (K-Fold)_ es la mejor opci√≥n si los medios lo permiten.
4. **Persistencia:** `joblib` permite guardar el modelo.
